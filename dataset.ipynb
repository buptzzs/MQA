{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:24:14.827926Z",
     "start_time": "2019-04-23T03:24:04.438360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "from typing import Dict, List\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.fields import Field, TextField, ListField, MetadataField, IndexField,ArrayField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Tokenizer, WordTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__) # pylint: disable=invalid-name\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")\n",
    "\n",
    "from allennlp.nn import util, InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.modules.matrix_attention import LinearMatrixAttention\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class QangarooReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    Reads a JSON-formatted Qangaroo file and returns a ``Dataset`` where the ``Instances`` have six\n",
    "    fields: ``candidates``, a ``ListField[TextField]``, ``query``, a ``TextField``, ``supports``, a\n",
    "    ``ListField[TextField]``, ``answer``, a ``TextField``, and ``answer_index``, a ``IndexField``.\n",
    "    We also add a ``MetadataField`` that stores the instance's ID and annotations if they are present.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer : ``Tokenizer``, optional (default=``WordTokenizer()``)\n",
    "        We use this ``Tokenizer`` for both the question and the passage.  See :class:`Tokenizer`.\n",
    "        Default is ```WordTokenizer()``.\n",
    "    token_indexers : ``Dict[str, TokenIndexer]``, optional\n",
    "        We similarly use this for both the question and the passage.  See :class:`TokenIndexer`.\n",
    "        Default is ``{\"tokens\": SingleIdTokenIndexer()}``.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 lazy: bool = False,\n",
    "                 use_label: bool = True,\n",
    "                 use_mention: bool = True) -> None:\n",
    "\n",
    "        super().__init__(lazy)\n",
    "        self._tokenizer = tokenizer or WordTokenizer()\n",
    "        self._token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer('token', True)}\n",
    "        self.use_label = use_label\n",
    "        self.use_mention = use_mention\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path: str):\n",
    "        # if `file_path` is a URL, redirect to the cache\n",
    "        file_path = cached_path(file_path)\n",
    "\n",
    "        logger.info(\"Reading file at %s\", file_path)\n",
    "        with open(file_path) as dataset_file:\n",
    "            dataset = json.load(dataset_file)\n",
    "        \n",
    "        logger.info('dataset length: %d',len(dataset))\n",
    "        logger.info(\"Reading the dataset\")\n",
    "        for sample in dataset:\n",
    "\n",
    "            instance = self.text_to_instance(sample['candidates'], sample['query'], sample['supports'],\n",
    "                                             sample['id'], sample['answer'],\n",
    "                                             sample['annotations'] if 'annotations' in sample else [[]])\n",
    "            if self.use_label:\n",
    "                if max(instance.fields['supports_labels'].array) == 0:\n",
    "                    continue\n",
    "            yield instance\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, # type: ignore\n",
    "                         candidates: List[str],\n",
    "                         query: str,\n",
    "                         supports: List[str],\n",
    "                         _id: str = None,\n",
    "                         answer: str = None,\n",
    "                         annotations: List[List[str]] = None) -> Instance:\n",
    "\n",
    "        # pylint: disable=arguments-differ\n",
    "        fields: Dict[str, Field] = {}\n",
    "\n",
    "        candidates_field = ListField([TextField(candidate, self._token_indexers)\n",
    "                                      for candidate in self._tokenizer.batch_tokenize(candidates)])\n",
    "\n",
    "        fields['query'] = TextField(self._tokenizer.tokenize(query.replace('_',' ')), self._token_indexers)\n",
    "\n",
    "        fields['supports'] = ListField([TextField(support, self._token_indexers)\n",
    "                                        for support in self._tokenizer.batch_tokenize(supports)])\n",
    "\n",
    "        fields['answer'] = TextField(self._tokenizer.tokenize(answer), self._token_indexers)\n",
    "\n",
    "        fields['answer_index'] = IndexField(candidates.index(answer), candidates_field)\n",
    "\n",
    "        fields['candidates'] = candidates_field\n",
    "\n",
    "        fields['metadata'] = MetadataField({'annotations': annotations, 'id': _id})\n",
    "        \n",
    "        if self.use_label:\n",
    "            answer_tokens = fields['answer'].tokens\n",
    "            answer_tokens = [token.text.lower() for token in answer_tokens]\n",
    "            answer_len = len(answer_tokens)\n",
    "            answer_str = ' '.join(answer_tokens)\n",
    "            supports_labels = []\n",
    "            for filed in fields['supports']:\n",
    "                tokens = filed.tokens\n",
    "                tokens = [ token.text.lower() for token in tokens]\n",
    "                is_support = 0\n",
    "                for i in range(len(tokens)-answer_len):\n",
    "                    token_add = ' '.join(tokens[i:i+answer_len])\n",
    "                    if token_add == answer_str:\n",
    "                        is_support = 1\n",
    "                        break\n",
    "                supports_labels.append(is_support)\n",
    "            fields['supports_labels'] = ArrayField(np.array(supports_labels))\n",
    "            \n",
    "        if self.use_mention:\n",
    "            all_mentions = []\n",
    "\n",
    "            for candidate_field in fields['candidates']:\n",
    "                candidate = candidate_field.tokens\n",
    "                candidate = [token.text.lower() for token in candidate]\n",
    "                c = ' '.join(candidate)\n",
    "                mentions = []\n",
    "                for idx, support_field in enumerate(fields['supports']):\n",
    "                    support = support_field.tokens\n",
    "                    support = [ token.text.lower() for token in support]\n",
    "                    for i in range(len(support)):\n",
    "                        token = support[i]\n",
    "                        if token == candidate[0]:\n",
    "                            s = ' '.join(support[i:i+len(candidate)])\n",
    "                            if s == c:\n",
    "                                mentions.append([idx, i, i+len(candidate)])\n",
    "                all_mentions.append(mentions)\n",
    "        fields['mentions'] = MetadataField(all_mentions)\n",
    "        \n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-23T03:30:34.086Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]INFO:__main__:Reading file at ./toy_data.json\n",
      "INFO:__main__:dataset length: 10\n",
      "INFO:__main__:Reading the dataset\n",
      "10it [00:00, 10.23it/s]\n",
      "0it [00:00, ?it/s]INFO:__main__:Reading file at ./data/qangaroo_v1.1/wikihop/dev.json\n",
      "INFO:__main__:dataset length: 5129\n",
      "INFO:__main__:Reading the dataset\n",
      "5099it [06:55, 12.26it/s]\n",
      "0it [00:00, ?it/s]INFO:__main__:Reading file at ./data/qangaroo_v1.1/wikihop/train.json\n",
      "INFO:__main__:dataset length: 43738\n",
      "INFO:__main__:Reading the dataset\n",
      "9567it [11:04, 10.60it/s]"
     ]
    }
   ],
   "source": [
    "reader = QangarooReader()\n",
    "train_path = './data/qangaroo_v1.1/wikihop/train.json'\n",
    "val_path = './data/qangaroo_v1.1/wikihop/dev.json'\n",
    "\n",
    "toy_data = reader.read('./toy_data.json')\n",
    "validation_dataset = reader.read(val_path)\n",
    "train_dataset = reader.read(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:16:26.579498Z",
     "start_time": "2019-04-23T03:16:26.552827Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "def test_mentions(dataset):\n",
    "    for idx in  trange(len(dataset)):\n",
    "        instance = dataset[idx]\n",
    "        mentions = instance.fields['mentions'].metadata\n",
    "        candidates = instance.fields['candidates']\n",
    "        answer_index = instance.fields['answer_index']\n",
    "        for idx, candidate_mentions in enumerate(mentions):\n",
    "            candidate = candidates[idx].tokens    \n",
    "            candidate = [token.text.lower() for token in candidate]    \n",
    "            candidate = ' '.join(candidate)\n",
    "            for mention in candidate_mentions:\n",
    "                support_idx, s_idx, e_idx = mention\n",
    "                support = instance.fields['supports'][support_idx].tokens\n",
    "                mention_item = support[s_idx:e_idx]\n",
    "                mention_item = [token.text.lower() for token in mention_item]    \n",
    "                mention_item = ' '.join(mention_item)\n",
    "                assert mention_item == candidate\n",
    "test_mentions(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
