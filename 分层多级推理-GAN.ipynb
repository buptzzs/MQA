{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多级推理模块\n",
    "\n",
    "0. 由于没有self-attention pooling了，所以再加一层self-attention层\n",
    "1. 每次更新段落的Summary vectors \n",
    "    input: [batch_sise, para_num, para_len, dim]\n",
    "    query: [batch_size, dim]\n",
    "    \n",
    "2. expand -> view -> biSeqAtt -> sum\n",
    "3. ori san"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:30:49.178249Z",
     "start_time": "2019-09-15T02:30:47.802586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from tensorboardX import SummaryWriter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torchtext.data import NestedField, Field, RawField\n",
    "from model import *\n",
    "from dataset import DataHandler\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:30:49.205379Z",
     "start_time": "2019-09-15T02:30:49.181541Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.hidden = 50\n",
    "        self.embedding_dim = 300\n",
    "        self.lr = 5e-4\n",
    "        self.epochs = 50\n",
    "        self.fix_length = 256\n",
    "        \n",
    "        self.log_dir = './logs'\n",
    "        self.model_name = 'gan'\n",
    "        self.batch_size = 4\n",
    "        #self.train_data = './data/train_filter.pt'\n",
    "        #self.dev_data = './data/dev_filter.pt'\n",
    "        self.train_data = './data/train_graph.pt'\n",
    "        self.dev_data = './data/dev_graph.pt'        \n",
    "        self.word_vocab = './data/glove_vocab.pt'\n",
    "        #self.word_vocab = None\n",
    "        #self.charNGram_vocab = None\n",
    "        \n",
    "        self.dropout = 0.2\n",
    "        self.seed = 1023\n",
    "        self.steps = 3\n",
    "        self.memory_type = 1\n",
    "        \n",
    "config = Config()\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:30:49.239964Z",
     "start_time": "2019-09-15T02:30:49.208728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:30:49.263460Z",
     "start_time": "2019-09-15T02:30:49.243211Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "torch.cuda.manual_seed_all(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:30:49.288013Z",
     "start_time": "2019-09-15T02:30:49.266107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./logs/gan_lr_0.0005__hidden__50_batchsize_4_p0.2_steps_3memory_type_1\n"
     ]
    }
   ],
   "source": [
    "save_path = config.model_name  + '_lr_'+ str(config.lr)+ '__hidden__' + str(config.hidden) \\\n",
    "            + '_batchsize_' + str(config.batch_size) +  '_p'+ str(config.dropout)+'_steps_'+str(config.steps)+'memory_type_' \\\n",
    "            + str(config.memory_type)\n",
    "save_path = os.path.join(config.log_dir, save_path)   \n",
    "print(save_path)\n",
    "config.save_path = save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Fileds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:30:54.074625Z",
     "start_time": "2019-09-15T02:30:50.853831Z"
    }
   },
   "outputs": [],
   "source": [
    "word_field = Field(batch_first=True, sequential=True, tokenize=\"spacy\", lower=True) # query\n",
    "multi_word_field = NestedField(word_field) \n",
    "\n",
    "word_field_sup = Field(batch_first=True, sequential=True, tokenize=\"spacy\", lower=True, fix_length=config.fix_length)\n",
    "multi_word_field_sup = NestedField(word_field_sup) \n",
    "\n",
    "charNGram_field = Field(batch_first=True, sequential=True, tokenize=\"spacy\", lower=True) # query\n",
    "multi_charNGram_field = NestedField(charNGram_field) \n",
    "\n",
    "charNGram_field_sup = Field(batch_first=True, sequential=True, tokenize=\"spacy\", lower=True, fix_length=config.fix_length)\n",
    "multi_charNGram_field_sup = NestedField(charNGram_field_sup) \n",
    "\n",
    "raw = RawField()\n",
    "raw.is_target = False\n",
    "\n",
    "label_field = Field(sequential=False, is_target=True, use_vocab=False)\n",
    "\n",
    "dict_field = {\n",
    "    'id': ('id', raw),\n",
    "    'supports': ('s_glove', multi_word_field_sup), \n",
    "    'query': ('q_glove', word_field), \n",
    "    'candidates': ('c_glove', multi_word_field),\n",
    "    'label': ('label', label_field),\n",
    "    'mentions': ('mentions', raw),\n",
    "    'para_label': ('para_label', raw),\n",
    "    'graph': ('graph', raw)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:31:43.814381Z",
     "start_time": "2019-09-15T02:30:54.078207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load examples.pt  :./data/train_graph.pt, ./data/dev_graph.pt\n"
     ]
    }
   ],
   "source": [
    "data_handler = DataHandler(config.train_data, config.dev_data, dict_field)\n",
    "\n",
    "# torch.save(data_handler.trainset.examples, './data/train_example.pt')\n",
    "# torch.save(data_handler.valset.examples, './data/dev_example.pt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T08:26:54.886481Z",
     "start_time": "2019-03-15T08:21:02.542403Z"
    },
    "collapsed": true
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def add_mentions(examples):\n",
    "\n",
    "    for example in tqdm(examples):\n",
    "        candidates = example.c_glove\n",
    "        supports = example.s_glove\n",
    "\n",
    "\n",
    "        all_mentions = []\n",
    "\n",
    "        for candidate in candidates:\n",
    "            mentions = []\n",
    "            c = ' '.join(candidate)\n",
    "            for idx, support in enumerate(supports):\n",
    "\n",
    "                for i in range(len(support)):\n",
    "                    token = support[i]\n",
    "                    if token == candidate[0]:\n",
    "                        s = ' '.join(support[i:i+len(candidate)])\n",
    "                        if s == c:\n",
    "                            mentions.append([idx, i, i+len(candidate)])\n",
    "            all_mentions.append(mentions)\n",
    "            \n",
    "        example.mentions = all_mentions\n",
    "        \n",
    "def add_para_label(examples):\n",
    "    filter_examples=  []\n",
    "    for example in tqdm(examples):\n",
    "        candidates = example.c_glove\n",
    "        supports = example.s_glove    \n",
    "\n",
    "        label = example.label\n",
    "        mentions = example.mentions\n",
    "        answser_mentions = mentions[label]\n",
    "        if len(answser_mentions) != 0:\n",
    "            para_label = [0]*len(supports)\n",
    "            for mentions in answser_mentions:\n",
    "                para_label[mentions[0]] = 1\n",
    "            example.para_label = para_label\n",
    "            filter_examples.append(example)\n",
    "    print(f'before filter: {len(examples)}, after:{len(filter_examples)}')\n",
    "    return filter_examples\n",
    "\n",
    "add_mentions(data_handler.valset.examples)\n",
    "add_mentions(data_handler.trainset.examples)\n",
    "\n",
    "train_filter_examples  = add_para_label(data_handler.trainset.examples)\n",
    "dev_filter_examples  = add_para_label(data_handler.valset.examples)\n",
    "\n",
    "torch.save(train_filter_examples, './data/train_filter.pt')\n",
    "torch.save(dev_filter_examples, './data/dev_filter.pt')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T16:00:28.597309Z",
     "start_time": "2019-06-03T15:37:28.743279Z"
    }
   },
   "source": [
    "def add_graph(examples):\n",
    "    for example in tqdm(examples):\n",
    "        batch_graph = []\n",
    "        candidate_num = len(example.c_glove)\n",
    "        support_num = len(example.s_glove)\n",
    "\n",
    "        mask = torch.zeros(candidate_num, support_num)\n",
    "        for i in range(len(example.mentions)):\n",
    "            candidate_mention = example.mentions[i]\n",
    "            for mention in candidate_mention:\n",
    "                mask[i][mention[0]] = 1\n",
    "        graph = torch.zeros(candidate_num, candidate_num)\n",
    "        for i in range(candidate_num):\n",
    "            for j in range(i+1,candidate_num):\n",
    "                graph[i][j] = (mask[j] * mask[i]).sum() > 0\n",
    "                graph[j][i] = (mask[j] * mask[i]).sum() > 0\n",
    "\n",
    "        example.graph = graph\n",
    "    \n",
    "\n",
    "add_graph(data_handler.valset.examples)\n",
    "add_graph(data_handler.trainset.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:31:43.840500Z",
     "start_time": "2019-09-15T02:31:43.817935Z"
    }
   },
   "outputs": [],
   "source": [
    "#torch.save(data_handler.valset.examples, './data/dev_graph.pt')\n",
    "#torch.save(data_handler.trainset.examples, './data/train_graph.pt')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:31:44.385220Z",
     "start_time": "2019-09-15T02:31:43.843323Z"
    }
   },
   "outputs": [],
   "source": [
    "if config.word_vocab is not None:\n",
    "    word_vocab = torch.load(config.word_vocab)\n",
    "    multi_word_field_sup.vocab = word_vocab\n",
    "    word_field_sup.vocab = word_vocab\n",
    "else:\n",
    "    multi_word_field_sup.build_vocab(data_handler.trainset, data_handler.valset, \n",
    "                                 vectors=torchtext.vocab.GloVe(dim=300,name='6B') )\n",
    "\n",
    "word_field.vocab = multi_word_field_sup.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:31:44.412753Z",
     "start_time": "2019-09-15T02:31:44.388323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([312667, 300])\n"
     ]
    }
   ],
   "source": [
    "print(multi_word_field_sup.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:44:06.583578Z",
     "start_time": "2019-05-21T13:42:05.405201Z"
    }
   },
   "source": [
    "multi_word_field_sup.build_vocab(data_handler.trainset, data_handler.valset, \n",
    "                         vectors=torchtext.vocab.GloVe(dim=300,name='840B') )\n",
    "torch.save(multi_word_field_sup.vocab, './data/glove_vocab.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:31:44.437749Z",
     "start_time": "2019-09-15T02:31:44.416240Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = data_handler.get_train_iter(batch_size=config.batch_size)\n",
    "val_iter = data_handler.get_val_iter(batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:31:44.477797Z",
     "start_time": "2019-09-15T02:31:44.440952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 4]\n",
       "\t[.id]:['WH_dev_0', 'WH_dev_1', 'WH_dev_2', 'WH_dev_3']\n",
       "\t[.s_glove]:[torch.LongTensor of size 4x15x256]\n",
       "\t[.q_glove]:[torch.LongTensor of size 4x11]\n",
       "\t[.c_glove]:[torch.LongTensor of size 4x18x4]\n",
       "\t[.label]:[torch.LongTensor of size 4]\n",
       "\t[.mentions]:[[[[6, 145, 146], [6, 173, 174], [7, 78, 79]], [[3, 50, 53], [5, 28, 31], [13, 1, 4]], [[6, 135, 136], [6, 218, 219], [6, 261, 262], [8, 45, 46], [12, 98, 99]], [[0, 2, 4], [7, 1, 3], [13, 64, 66], [13, 69, 71]], [[0, 36, 38], [10, 1, 3], [13, 75, 77]], [[0, 14, 15], [1, 63, 64], [1, 138, 139], [1, 186, 187], [1, 238, 239], [2, 128, 129], [9, 8, 9], [9, 43, 44], [10, 19, 20], [10, 34, 35], [11, 37, 38], [11, 79, 80], [13, 56, 57]], [[7, 37, 40]], [[6, 180, 181], [12, 101, 102]], [[12, 96, 97]], [[8, 43, 46]], [[6, 169, 172]], [[6, 179, 181]], [[7, 38, 40]], [[6, 147, 148], [6, 182, 183]], [[6, 171, 172], [9, 6, 7], [11, 35, 36], [11, 121, 122]], [[2, 125, 127], [8, 8, 10], [12, 89, 91]], [[1, 0, 2], [1, 100, 102], [1, 141, 143], [1, 167, 169], [5, 17, 19], [10, 41, 43], [13, 94, 96]], [[1, 132, 133], [2, 161, 162], [2, 173, 174], [2, 235, 236], [2, 260, 261], [3, 91, 92], [3, 159, 160], [4, 0, 1], [4, 14, 15], [12, 41, 42], [13, 85, 86], [14, 0, 1], [14, 18, 19], [14, 45, 46], [14, 187, 188]]], [[[5, 37, 39]], [[1, 9, 10], [2, 28, 29], [3, 22, 23], [3, 60, 61], [3, 138, 139], [7, 3, 4], [7, 30, 31], [7, 41, 42], [7, 65, 66]], [[4, 30, 32]], [[0, 27, 29], [4, 35, 37]]], [[[3, 101, 104], [5, 8, 11], [5, 94, 97]], [[4, 49, 53]], [[1, 79, 80], [2, 34, 35], [7, 26, 27]]], [[[0, 5, 7]], [[0, 6, 7], [1, 20, 21], [1, 29, 30], [1, 81, 82], [2, 19, 20], [3, 15, 16], [3, 50, 51], [4, 22, 23]]]]\n",
       "\t[.para_label]:[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 1, 0, 0], [1, 0, 0, 0, 0]]\n",
       "\t[.graph]:[tensor([[0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.],\n",
       "        [1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.],\n",
       "        [0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.]]), tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.]]), tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]), tensor([[0., 1.],\n",
       "        [1., 0.]])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, batch in enumerate(val_iter):\n",
    "    break\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:31:44.547678Z",
     "start_time": "2019-09-15T02:31:44.480627Z"
    }
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "class GAN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, dropout):\n",
    "        super(GAN, self).__init__()\n",
    "        self.gan = GraphAttentionLayer(in_features, out_features, dropout,0.1)\n",
    "          \n",
    "        \n",
    "    def forward(self, input, graph):\n",
    "        \n",
    "        batch_size = input.shape[0]\n",
    "        outs = []\n",
    "        for i in range(batch_size):\n",
    "            out = self.gan(input[i], graph[i])\n",
    "            outs.append(out)\n",
    "        return torch.stack(outs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:31:44.581430Z",
     "start_time": "2019-09-15T02:31:44.550581Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_graph(batch):\n",
    "    candidate_num = batch.c_glove.shape[1]\n",
    "    support_num = batch.s_glove.shape[1]\n",
    "    batch_size = batch.c_glove.shape[0]\n",
    "    batch_graph = torch.zeros(batch_size,candidate_num,candidate_num)\n",
    "    for i in range(batch_size):\n",
    "        graph = batch.graph[i]\n",
    "        n = graph.shape[0]\n",
    "        batch_graph[i,:n,:n] = graph\n",
    "    return batch_graph      \n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, hidden, dropout=0.2):\n",
    "        super(GNN, self).__init__()\n",
    "        self.W_1 = nn.Linear(hidden, hidden)\n",
    "        self.W_2 = nn.Linear(hidden, hidden)\n",
    "        self.W_g = nn.Linear(hidden*2, hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        '''\n",
    "        input:\n",
    "        x :[batch, candidate_num, hidden]\n",
    "           adj: [batch, candidate_num, candidate_num]\n",
    "           \n",
    "        '''\n",
    "        x1 = self.dropout(self.W_1(x))\n",
    "        x2 = self.dropout(self.W_2(x))\n",
    "        \n",
    "        x2 = torch.bmm(adj, x)\n",
    "        \n",
    "        u = x1 + x2\n",
    "        g = self.W_g(torch.cat([x,u],dim=-1))\n",
    "        g = torch.sigmoid(g)\n",
    "        g = self.dropout(g)\n",
    "        \n",
    "        x = torch.tanh(u)*g + x * (1-g)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:31:44.630672Z",
     "start_time": "2019-09-15T02:31:44.584377Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_mask(x_size, num_turn, dropout_p=0.0, is_training=False):\n",
    "    if not is_training: dropout_p = 0.0\n",
    "    new_data = torch.zeros(x_size, num_turn)\n",
    "    new_data = (1-dropout_p) * (new_data.zero_() + 1)\n",
    "    for i in range(new_data.size(0)):\n",
    "        one = random.randint(0, new_data.size(1)-1)\n",
    "        new_data[i][one] = 1\n",
    "    mask = 1.0/(1 - dropout_p) * torch.bernoulli(new_data)\n",
    "    mask.requires_grad = False\n",
    "    return mask\n",
    "\n",
    "class SAN(nn.Module):\n",
    "    def __init__(self, question_dim, support_dim, candidate_dim, num_turn=5, dropout=0.2, memo_dropout=0.4, memory_type=0, gan_dropout=0.5,\n",
    "                 device=None):\n",
    "        super(SAN,self).__init__()\n",
    "        self.qp_bilinear_attention_word = BilinearSeqAttn(support_dim, question_dim, dropout=dropout)\n",
    "        self.qp_bilinear_attention_para = BilinearSeqAttn(support_dim, question_dim, dropout=dropout)\n",
    "\n",
    "        self.candidates_scorer = BilinearSeqAttn(candidate_dim, question_dim, dropout=dropout)        \n",
    "        self.gru = nn.GRUCell(support_dim, question_dim)\n",
    "        self.gnn = GNN(candidate_dim,gan_dropout)\n",
    "        \n",
    "        self.num_turn = num_turn\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.memo_dropout=memo_dropout\n",
    "        self.device = device\n",
    "        self.memory_type = memory_type\n",
    "        \n",
    "    def forward(self, question_embedding, para_embedding, candidates_embedding, para_length, graph=None):\n",
    "        '''\n",
    "        input:\n",
    "            question_embedding: [batch_size, hidden_dim]\n",
    "            para_embedding: [batch_size*para_num, para_length, hidden_dim]\n",
    "            candidates_embedding: [batch_size, candidates_num, hidden_dim]\n",
    "\n",
    "        '''\n",
    "        score_list = []\n",
    "        batch_size = question_embedding.size(0)\n",
    "        hidden = question_embedding.size(1)        \n",
    "        for turn in range(self.num_turn):\n",
    "            question_embedding_expand = question_embedding.unsqueeze(1).expand(batch_size, para_length, hidden).contiguous()\n",
    "            question_embedding_expand = question_embedding_expand.view(-1,hidden)    \n",
    "            \n",
    "            # update paragraph embedding\n",
    "            qp_score_word = self.qp_bilinear_attention_word(para_embedding, question_embedding_expand)\n",
    "            qp_score_word = F.softmax(qp_score_word, 1)\n",
    "            para_embedding_summary = torch.bmm(qp_score_word.unsqueeze(1), para_embedding).squeeze(1)\n",
    "            para_embedding_summary = para_embedding_summary.contiguous().view(batch_size, para_length, hidden)\n",
    "            \n",
    "            # update question embedding\n",
    "            qp_score_para = self.qp_bilinear_attention_para(para_embedding_summary, question_embedding)\n",
    "            qp_score_para = F.softmax(qp_score_para, 1)\n",
    "            S = torch.bmm(qp_score_para.unsqueeze(1), para_embedding_summary).squeeze(1)\n",
    "            \n",
    "            S = self.dropout(S)\n",
    "            question_embedding = self.gru(S, question_embedding)\n",
    "            \n",
    "            # Graph update\n",
    "            if graph is not None:\n",
    "                candidates_embedding = self.gnn(candidates_embedding, graph)\n",
    "            \n",
    "            # compute candidates score            \n",
    "            candidates_score = self.candidates_scorer(candidates_embedding, question_embedding)\n",
    "\n",
    "            score_list.append(candidates_score)\n",
    "        if self.memory_type == 0:\n",
    "            mask = generate_mask(batch_size,self.num_turn, self.memo_dropout, self.training)\n",
    "            mask = mask.to(self.device)\n",
    "            mask = [m.contiguous() for m in torch.unbind(mask, 1)]\n",
    "\n",
    "            score_list = [mask[idx].view(batch_size, 1).expand_as(inp) * inp for idx, inp in enumerate(score_list)]\n",
    "            scores = torch.stack(score_list, 2)\n",
    "            scores = torch.mean(scores, 2)\n",
    "        elif self.memory_type == 1:\n",
    "            scores = torch.stack(score_list, 2)\n",
    "            scores = torch.mean(scores, 2)\n",
    "        elif self.memory_type == 2:\n",
    "            scores = score_list[-1]            \n",
    "            \n",
    "        return scores\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:31:50.301779Z",
     "start_time": "2019-09-15T02:31:50.146436Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SimpleQANet(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, word_vectors, device):\n",
    "        super(SimpleQANet, self).__init__()\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        self.embedding_layer = EmbeddingLayer(word_vectors)\n",
    "        \n",
    "\n",
    "        self.rnn = EncoderRNN(config.embedding_dim, config.hidden, 1, True, True, config.dropout, False)\n",
    "                \n",
    "            \n",
    "        self.co_att = CoAttention(config.hidden*2, att_type=2, dropout=config.dropout)\n",
    "        \n",
    "        self.linear_1 = nn.Sequential(\n",
    "                        nn.Linear(config.hidden*4, config.hidden),\n",
    "                        nn.ReLU()\n",
    "                    )        \n",
    "        self.rnn2 =  EncoderRNN(config.hidden, config.hidden, 1, True, True, config.dropout, False)\n",
    "        \n",
    "        self.word_att = SelfAttention(config.hidden*2, config.hidden*2, config.dropout)\n",
    "        self.word_att_q = SelfAttention(config.hidden*2, config.hidden*2, config.dropout)\n",
    "        \n",
    "        self.pass_att = SelfAttention(config.hidden*2, config.hidden*2, config.dropout)\n",
    "        \n",
    "        self.c_att = SelfAttention(config.hidden*2, config.hidden*2, config.dropout)\n",
    "                \n",
    "        \n",
    "        #self.fusion = FusionLayer(config.hidden*2, dropout=config.dropout)\n",
    "        self.max_pooling = PoolingLayer()     \n",
    "        \n",
    "        self.fc = nn.Linear(config.hidden*2, config.hidden*4)\n",
    "        self.san = SAN(config.hidden*2,config.hidden*2,config.hidden*6, num_turn=config.steps, memory_type=config.memory_type, device=device)\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def get_candidate_vectors(self, batch, support_vectors, device):\n",
    "        batch_size, candidate_num,_ = batch.c_glove.shape\n",
    "        _,support_num, support_length = batch.s_glove.shape\n",
    "        hidden = support_vectors.shape[-1]\n",
    "\n",
    "        masks = []\n",
    "        for idx, candidate_mentions in enumerate(batch.mentions):\n",
    "            mask = torch.zeros(candidate_num, support_num, support_length)\n",
    "            for i in range(len(candidate_mentions)):\n",
    "                candidate_mention = candidate_mentions[i]\n",
    "                for mention in candidate_mention:\n",
    "                    mask[i][mention[0]][mention[1]:mention[2]] = 1\n",
    "            masks.append(mask)\n",
    "        masks = torch.stack(masks).to(device)\n",
    "\n",
    "        support_vectors = support_vectors.view(batch_size,-1,hidden).unsqueeze(1)\n",
    "\n",
    "        masks = masks.view(batch_size,candidate_num,-1)\n",
    "        masks_expand = masks.unsqueeze(-1).expand(batch_size, candidate_num, support_length*support_num, hidden)\n",
    "        \n",
    "        candidates = support_vectors * masks_expand\n",
    "        \n",
    "        candidates_max = candidates.max(-2)[0]\n",
    "        candidates_mean = torch.mean(candidates,-2)\n",
    "        candidates_vectors = torch.cat([candidates_max, candidates_mean],-1)    \n",
    "\n",
    "        return candidates_vectors        \n",
    "        \n",
    "    def forward(self, batch, return_label = True):\n",
    "        if type(batch.q_glove) is tuple:\n",
    "            q_glove, _ = batch.q_glove\n",
    "        else:\n",
    "            q_glove = batch.q_glove\n",
    "        s_glove = batch.s_glove\n",
    "        c_glove = batch.c_glove\n",
    "        \n",
    "        q_glove = q_glove.to(self.device)\n",
    "        s_glove = s_glove.to(self.device)\n",
    "        c_glove = c_glove.to(self.device)        \n",
    "        \n",
    "        q_out = self.embedding_layer(q_glove) # [batch_size,qeustion_length, hidden_dim]\n",
    "        s_out = self.embedding_layer(s_glove) # [batch_szie, support_num, support_length, hidden_dim]\n",
    "        c_out = self.embedding_layer(c_glove) # [batch_size, candidates_num, candidates_length, hidden_dim]        \n",
    "        \n",
    "        batch_size=  s_out.size(0)\n",
    "        \n",
    "        s_len = s_out.size(1)\n",
    "        c_len = c_out.size(1)\n",
    "        \n",
    "        s_word_len = s_out.size(2)\n",
    "        c_word_len = c_out.size(2)\n",
    "        \n",
    "        hidden = s_out.size(-1)\n",
    "        \n",
    "        s_out = s_out.view(batch_size*s_len, s_word_len, hidden).contiguous()\n",
    "        c_out = c_out.view(batch_size*c_len, c_word_len, hidden).contiguous()\n",
    "        \n",
    "        q_out = self.rnn(q_out) # [batch_size,qeustion_length, hidden_dim]\n",
    "        c_out = self.rnn(c_out) # [batch_szie * support_num, support_length, hidden_dim]\n",
    "        s_out = self.rnn(s_out) # [batch_size * candidates_num, candidates_length, hidden_dim] \n",
    "        \n",
    "        # Attention\n",
    "        \n",
    "        q_word_len = q_out.size(1)\n",
    "        q_out_expand = q_out.unsqueeze(1).expand(batch_size, s_len, q_word_len, q_out.size(-1)).contiguous()\n",
    "        q_out_expand = q_out_expand.view(batch_size*s_len, q_word_len, q_out.size(-1)).contiguous()\n",
    "        \n",
    "        s_out_att, q_out_att = self.co_att(s_out, q_out_expand)\n",
    "        #S_s = self.fusion(s_out, s_out_att)\n",
    "        #S_q = self.fusion(q_out, q_out_att)\n",
    "        \n",
    "        S_s = self.linear_1(s_out_att)\n",
    "        S_s = self.rnn2(S_s) # [batch_size * para_num, para_length, hidden*2]\n",
    "        \n",
    "        \n",
    "        candidates_vectors = self.get_candidate_vectors(batch, S_s, self.device)\n",
    "        question_summary = self.word_att_q(q_out)\n",
    "        \n",
    "        \n",
    "        candidates_summary = self.c_att(c_out)        \n",
    "        candidates_summary = candidates_summary.view(batch_size, c_len, -1)\n",
    "        \n",
    "        candidates_summary = torch.cat([candidates_summary, candidates_vectors],-1)\n",
    "        \n",
    "        \n",
    "        #graph = generate_graph(batch).to(device)\n",
    "        graph = None\n",
    "        score = self.san(question_summary, S_s, candidates_summary, s_len, graph)\n",
    "        \n",
    "        candidates_mask = ((c_glove > 1).sum(-1) > 0).float().to(device)\n",
    "        score = score * candidates_mask + (-1e15)*(1-candidates_mask)      \n",
    "        \n",
    "        if return_label:\n",
    "            label = batch.label.to(self.device)\n",
    "            return score, label\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:31:55.727282Z",
     "start_time": "2019-09-15T02:31:50.973297Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = SimpleQANet(config, word_field.vocab.vectors, device)\n",
    "#score, label= model(batch)\n",
    "#print(score.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:31:55.753751Z",
     "start_time": "2019-09-15T02:31:55.730733Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:31:56.201669Z",
     "start_time": "2019-09-15T02:31:55.756980Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import AverageMeter\n",
    "\n",
    "def train(epoch, data_iter, model, criterion, optimizer, batch_size=1):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.train()\n",
    "    #model.embedding_layer.eval()\n",
    "    with trange(len(data_iter)) as t:\n",
    "        for idx, batch in enumerate(data_iter):\n",
    "            score, label, = model(batch)\n",
    "\n",
    "            loss = criterion(score, label)\n",
    "\n",
    "            loss = loss / batch_size\n",
    "            loss.backward()\n",
    "            if (idx+1)%batch_size == 0 :\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)            \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()        \n",
    "\n",
    "            losses.update(loss.item()*batch_size)\n",
    "\n",
    "            pred = score.argmax(1)\n",
    "            acc = pred.eq(label).sum().item()  / pred.size(0)\n",
    "            acces.update(acc)\n",
    "            \n",
    "            matrix = {\n",
    "                'acc':acces.avg,\n",
    "                'epoch':epoch,\n",
    "                'loss': losses.avg\n",
    "            }\n",
    "            t.set_postfix(matrix)\n",
    "            t.update()\n",
    "            if (idx+1) % (batch_size*100) == 0:\n",
    "                print(f'epoch:{epoch}, idx:{idx}/{len(data_iter)}, loss:{losses.avg}, acc:{acces.avg}')\n",
    "    return losses.avg, acces.avg\n",
    "\n",
    "def val(epoch, data_iter, model, criterion):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.eval()\n",
    "    for idx, batch in enumerate(data_iter):\n",
    "        with torch.no_grad():\n",
    "            score, label = model(batch)\n",
    "                    \n",
    "        loss = criterion(score, label)\n",
    "\n",
    "        losses.update(loss.item())\n",
    "        \n",
    "        pred = score.argmax(1)\n",
    "        acc = pred.eq(label).sum().item()  / pred.size(0)\n",
    "        acces.update(acc)\n",
    "        if idx % 100 == 0:\n",
    "            print(f'epoch:{epoch}, idx:{idx}/{len(data_iter)}, loss:{losses.avg}, acc:{acces.avg}')\n",
    "    return losses.avg, acces.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-15T02:40:31.605Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                             lr=config.lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
    "#train(0, train_iter, model, criterion, optimizer, batch_size=config.batch_size)\n",
    "# val(0, val_iter, model,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-15T02:40:32.053Z"
    }
   },
   "outputs": [],
   "source": [
    "cycle_len = 1\n",
    "cycle_iter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-15T02:40:32.448Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10845 [00:00<?, ?it/s]/home/zzs/Multi_Evidence_QA/model.py:181: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alphas = self.softmax(alphas)  # (bsz, sent_len)\n",
      "  1%|          | 100/10845 [00:24<42:42,  4.19it/s, acc=0.188, epoch=0, loss=2.44]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:99/10845, loss:2.440446113348007, acc:0.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 200/10845 [00:48<42:47,  4.15it/s, acc=0.251, epoch=0, loss=2.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:199/10845, loss:2.3479348433017733, acc:0.25125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 300/10845 [01:12<43:39,  4.03it/s, acc=0.277, epoch=0, loss=2.29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:299/10845, loss:2.2941722665230433, acc:0.27666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 400/10845 [01:36<41:13,  4.22it/s, acc=0.294, epoch=0, loss=2.24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:399/10845, loss:2.2385188657045365, acc:0.29375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 500/10845 [02:00<40:32,  4.25it/s, acc=0.3, epoch=0, loss=2.22]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:499/10845, loss:2.223029584169388, acc:0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 600/10845 [02:24<39:55,  4.28it/s, acc=0.306, epoch=0, loss=2.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:599/10845, loss:2.192085688014825, acc:0.30583333333333335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 700/10845 [02:49<42:00,  4.03it/s, acc=0.312, epoch=0, loss=2.18]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:699/10845, loss:2.179809968812125, acc:0.31214285714285717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 800/10845 [03:13<42:10,  3.97it/s, acc=0.317, epoch=0, loss=2.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:799/10845, loss:2.1671188152208924, acc:0.316875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 900/10845 [03:37<40:44,  4.07it/s, acc=0.326, epoch=0, loss=2.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:899/10845, loss:2.138180994656351, acc:0.3258333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1000/10845 [04:01<37:34,  4.37it/s, acc=0.333, epoch=0, loss=2.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:999/10845, loss:2.1191528463363647, acc:0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1100/10845 [04:25<39:54,  4.07it/s, acc=0.337, epoch=0, loss=2.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:1099/10845, loss:2.1001409003815867, acc:0.33704545454545454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1200/10845 [04:50<38:47,  4.14it/s, acc=0.339, epoch=0, loss=2.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:1199/10845, loss:2.0968695268407465, acc:0.3385416666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1300/10845 [05:14<36:48,  4.32it/s, acc=0.344, epoch=0, loss=2.08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:1299/10845, loss:2.082605100996219, acc:0.34423076923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1400/10845 [05:38<41:20,  3.81it/s, acc=0.349, epoch=0, loss=2.07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:1399/10845, loss:2.069813695624471, acc:0.34875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1500/10845 [06:02<37:24,  4.16it/s, acc=0.354, epoch=0, loss=2.06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:1499/10845, loss:2.0574700970550377, acc:0.354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1600/10845 [06:27<37:28,  4.11it/s, acc=0.356, epoch=0, loss=2.05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:1599/10845, loss:2.045000130040571, acc:0.35625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1700/10845 [06:52<38:41,  3.94it/s, acc=0.358, epoch=0, loss=2.04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:1699/10845, loss:2.044044786788085, acc:0.35823529411764704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1800/10845 [07:16<35:18,  4.27it/s, acc=0.361, epoch=0, loss=2.03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:1799/10845, loss:2.032346332594752, acc:0.36125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1900/10845 [07:40<37:57,  3.93it/s, acc=0.362, epoch=0, loss=2.02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:1899/10845, loss:2.0236138432198447, acc:0.36236842105263156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2000/10845 [08:05<37:18,  3.95it/s, acc=0.365, epoch=0, loss=2.01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:1999/10845, loss:2.0144051858708263, acc:0.3655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 2100/10845 [08:29<33:37,  4.33it/s, acc=0.367, epoch=0, loss=2.01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:2099/10845, loss:2.0069683878691422, acc:0.36714285714285716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2200/10845 [08:53<36:17,  3.97it/s, acc=0.37, epoch=0, loss=2]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:2199/10845, loss:1.9970509629290212, acc:0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2300/10845 [09:17<33:17,  4.28it/s, acc=0.371, epoch=0, loss=1.99]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, idx:2299/10845, loss:1.9878806694111097, acc:0.3714130434782609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2341/10845 [09:27<33:04,  4.29it/s, acc=0.372, epoch=0, loss=1.98]"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(config.save_path):\n",
    "    os.makedirs(config.save_path)\n",
    "writer = SummaryWriter(config.save_path)\n",
    "\n",
    "best_acc = 0.0\n",
    "for i in range(cycle_len):\n",
    "    optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                             lr=config.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cycle_iter)\n",
    "    for epoch in range(cycle_iter):\n",
    "        scheduler.step()\n",
    "        train_loss, train_acc = train(epoch, train_iter, model, criterion, optimizer, 1)\n",
    "        val_loss, val_acc = val(epoch, val_iter, model, criterion)\n",
    "        global_epoch = cycle_iter * i + epoch + 1\n",
    "        writer.add_scalar('train_loss', train_loss, global_epoch)\n",
    "        writer.add_scalar('val_loss', val_loss, global_epoch)\n",
    "        writer.add_scalar('train_acc', train_acc, global_epoch)\n",
    "        writer.add_scalar('val_acc', val_acc, global_epoch)\n",
    "\n",
    "        state = {\n",
    "            'val_acc': val_acc,\n",
    "            'train_acc': train_acc,\n",
    "            'epoch': epoch\n",
    "            ,\n",
    "            'model': model.state_dict()\n",
    "        }\n",
    "        torch.save(state, os.path.join(config.save_path,'lastest.pth'))\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(state, os.path.join(save_path, f'best_epoch{epoch}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T14:40:44.765789Z",
     "start_time": "2019-09-14T14:48:30.927Z"
    }
   },
   "outputs": [],
   "source": [
    "score, label= model(batch)\n",
    "print(score.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T14:40:44.767174Z",
     "start_time": "2019-09-14T14:48:30.931Z"
    }
   },
   "outputs": [],
   "source": [
    "batch.c_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T14:40:44.768540Z",
     "start_time": "2019-09-14T14:48:30.935Z"
    }
   },
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
