{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T06:38:43.344492Z",
     "start_time": "2019-01-03T06:38:43.330280Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from dataset import DataHandler, BertField\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from model import BiAttention, EncoderRNN, SelfAttention\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T06:35:42.961553Z",
     "start_time": "2019-01-03T06:35:42.955685Z"
    }
   },
   "outputs": [],
   "source": [
    "train_examples_path = './train_examples.pt'\n",
    "val_examples_path = './val_examples.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T06:35:43.304862Z",
     "start_time": "2019-01-03T06:35:43.207984Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./bert-base-uncased-vocab.txt')\n",
    "\n",
    "bert_field = BertField(tokenizer)\n",
    "multi_bert_field = data.NestedField(bert_field)\n",
    "\n",
    "word_field = data.Field(batch_first=True, sequential=True, tokenize=tokenizer.tokenize, lower=True) # query\n",
    "multi_word_field = data.NestedField(word_field) \n",
    "\n",
    "raw = data.RawField()\n",
    "raw.is_target = False\n",
    "\n",
    "label_field = data.Field(sequential=False, is_target=True, use_vocab=False)\n",
    "\n",
    "dict_field = {\n",
    "    'id': ('id', raw),\n",
    "    'supports': [('s_glove', multi_word_field), ('s_bert', multi_bert_field)],\n",
    "    'query': [('q_glove', word_field), ('q_bert', bert_field)],\n",
    "    'answer': [('a_glove', word_field), ('a_bert', bert_field)],\n",
    "    'candidates': [('c_glove', multi_word_field), ('c_bert', multi_bert_field)],\n",
    "    'label': ('label', label_field)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T06:36:30.022809Z",
     "start_time": "2019-01-03T06:35:44.746968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load examples.pt  :./train_examples.pt, ./val_examples.pt\n"
     ]
    }
   ],
   "source": [
    "data_handler = DataHandler(train_examples_path, val_examples_path, dict_field, word_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T06:36:30.029570Z",
     "start_time": "2019-01-03T06:36:30.025764Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = data_handler.get_train_iter(batch_size=1)\n",
    "val_iter = data_handler.get_val_iter(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T06:36:30.045580Z",
     "start_time": "2019-01-03T06:36:30.032315Z"
    }
   },
   "outputs": [],
   "source": [
    "for batch in val_iter:\n",
    "    input, target = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "这一层需要频繁的改动，所以暂时不放在py文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T06:38:03.225988Z",
     "start_time": "2019-01-03T06:38:03.207336Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, word_field, bert_model_path='./bert-base-uncased/', use_all=False):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.word_embedding_layer = nn.Embedding.from_pretrained(embeddings=word_field.vocab.vectors)\n",
    "        self.word_embedding_layer.eval()\n",
    "        \n",
    "        model = BertModel.from_pretrained(bert_model_path)   \n",
    "        self.bert_model = model\n",
    "        \n",
    "        self.use_all = use_all\n",
    "        self.freeze()\n",
    "        \n",
    "    def freeze(self):\n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.word_embedding_layer.weight.requires_grad = False\n",
    "        \n",
    "    def forward(self, word_tokens, bert_tokens):\n",
    "        '''\n",
    "        input:\n",
    "            x: [batch_size, seg_len]\n",
    "        \n",
    "        return embeddings: [batch_size, seq_len, glove_dim + bert_dim]    \n",
    "        '''\n",
    "        word_embeddings = self.word_embedding_layer(word_tokens)\n",
    "        \n",
    "        # encoded_layers: [batch_size, seq_len, bert_embedding_dim] * num_of_layers\n",
    "        self.bert_model.eval()\n",
    "        encoded_layers, _ = self.bert_model(bert_tokens)\n",
    "        \n",
    "        bert_embeddings = torch.zeros_like(encoded_layers[-1])\n",
    "        if self.use_all:\n",
    "            for layer in encoded_layers:\n",
    "                bert_embeddings += layer\n",
    "            bert_embeddings /= len(encoded_layers)\n",
    "        else:\n",
    "            bert_embeddings += encoded_layers[-1]\n",
    "        \n",
    "        out = torch.cat([word_embeddings, bert_embeddings], dim=-1)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T06:38:47.711577Z",
     "start_time": "2019-01-03T06:38:47.670452Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleQANet(nn.Module):\n",
    "    \n",
    "    def __init__(self, config, word_field):\n",
    "        super(SimpleQANet, self).__init__()\n",
    "        self.config = config\n",
    "        self.use_cuda = config.use_cuda\n",
    "        \n",
    "        self.embedding_layer = EmbeddingLayer(word_field, config.bert_path, config.use_all)\n",
    "        self.rnn = EncoderRNN(config.word_dim + config.bert_dim, config.hidden, 1, True, True, 0.2, False)\n",
    "        \n",
    "        self.qc_att = BiAttention(config.hidden*2, 0.2)\n",
    "        self.linear_1 = nn.Sequential(\n",
    "                nn.Linear(config.hidden*8, config.hidden),\n",
    "                nn.ReLU()\n",
    "        )    \n",
    "        \n",
    "        self.rnn_2 = EncoderRNN(config.hidden, config.hidden, 1, False, True, 0.2, False)\n",
    "        \n",
    "        self.self_att = SelfAttention(config.hidden*2, config.hidden*2, 0.2)       \n",
    "        self.self_att_2 = SelfAttention(config.hidden*2, config.hidden*2, 0.2)        \n",
    "        \n",
    "        self.self_att_c = SelfAttention(config.hidden*2, config.hidden*2, 0.2)        \n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        q_glove, _= batch.q_glove\n",
    "        q_bert = batch.q_bert\n",
    "        s_glove = batch.s_glove\n",
    "        s_bert = batch.s_bert\n",
    "        c_glove = batch.c_glove\n",
    "        c_bert = batch.c_bert\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            q_glove = q_glove.cuda()\n",
    "            q_bert = q_bert.cuda()\n",
    "            s_glove = s_glove.cuda()\n",
    "            s_bert = s_bert.cuda()\n",
    "            c_glove = c_glove.cuda()\n",
    "            c_bert = c_bert.cuda()\n",
    "        \n",
    "        # Embedding \n",
    "        q_out = self.embedding_layer(q_glove, q_bert)\n",
    "        s_out = self.embedding_layer(s_glove.squeeze(), s_bert.squeeze())\n",
    "        c_out = self.embedding_layer(c_glove.squeeze(), c_bert.squeeze())\n",
    "\n",
    "        q_out = self.rnn(q_out)\n",
    "        c_out = self.rnn(c_out)\n",
    "        s_out = self.rnn(s_out)\n",
    "\n",
    "        # bi-attention on supports and  question\n",
    "        context_mask = (c_bert.squeeze() > 0).float()\n",
    "        ques_mask = (q_bert > 0).float()\n",
    "        \n",
    "        support_len = s_out.size(0)\n",
    "        q_out = q_out.expand(support_len, q_out.size(1), q_out.size(2))\n",
    "        ques_mask = ques_mask.expand(support_len, q_out.size(1))        \n",
    "        \n",
    "        # s_out:[supports_len, seq_len, hidden*2], q_out: [support_len, seq_len, hidden*2]\n",
    "        output = self.qc_att(s_out, q_out, ques_mask)\n",
    "        output = self.linear_1(output)\n",
    "        output = self.rnn_2(output)\n",
    "        \n",
    "        # self-attention pooling \n",
    "        # [support_len, hidden*2]\n",
    "        output = self.self_att(output)\n",
    "        # [1, hidden*2]\n",
    "        output = self.self_att_2(output.unsqueeze(0))\n",
    "\n",
    "        # [candidate_len, hidden*2]\n",
    "        c_out = self.self_att_c(c_out)\n",
    "        \n",
    "        # score layer\n",
    "        score = torch.mm(c_out, torch.tanh(output.transpose(0, 1)))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T07:40:42.291723Z",
     "start_time": "2019-01-03T07:40:42.279355Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hidden = 100\n",
    "        self.word_dim = 300\n",
    "        self.bert_dim = 768\n",
    "        self.use_cuda = False\n",
    "        self.bert_path = './bert-base-uncased/'\n",
    "        self.use_all = True\n",
    "        self.lr = 1e-4\n",
    "        self.epochs = 1\n",
    "        self.log_dir = './logs'\n",
    "        self.model_name = 'simpleQANet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T07:40:42.448787Z",
     "start_time": "2019-01-03T07:40:42.444130Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "#model = SimpleQANet(config, word_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T07:13:09.575157Z",
     "start_time": "2019-01-03T07:13:01.979758Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzs/Multi_Evidence_QA/model.py:150: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alphas = self.softmax(alphas)  # (bsz, sent_len)\n"
     ]
    }
   ],
   "source": [
    "out = model(batch)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "target = batch.label\n",
    "out = out.transpose(0,1)\n",
    "loss = criterion(out, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T07:16:58.371886Z",
     "start_time": "2019-01-03T07:16:58.360467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = out.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T07:16:24.846998Z",
     "start_time": "2019-01-03T07:16:24.839112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.eq(target).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T07:11:43.311380Z",
     "start_time": "2019-01-03T07:11:43.300415Z"
    }
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T07:36:09.967085Z",
     "start_time": "2019-01-03T07:36:09.948834Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch, data_iter, model, criterion, optimizer, cuda):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.train()\n",
    "    model.embedding_layer.eval()\n",
    "    for idx, batch in enumerate(data_iter):\n",
    "        score = model(batch)\n",
    "        label = batch.label\n",
    "        if cuda:\n",
    "            label = label.cuda()\n",
    "        score = score.transpose(0,1)      \n",
    "        \n",
    "        loss = criterion(score, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.item())\n",
    "        \n",
    "        pred = score.argmax(1)\n",
    "        acc = pred.eq(target).sum().item()   \n",
    "        acces.update(acc)\n",
    "        break\n",
    "        print(f'epoch:{epoch}, idx:{idx}/{len(data_iter)}, loss:{losses.avg}, acc:{acces.avg}')\n",
    "    return losses.avg, acces.avg\n",
    "\n",
    "def val(epoch, data_iter, model, criterion, cuda):\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    model.eval()\n",
    "    for idx, batch in enumerate(data_iter):\n",
    "        with torch.no_grad():\n",
    "            score = model(batch)\n",
    "            \n",
    "        label = batch.label\n",
    "        if cuda:\n",
    "            label = label.cuda()\n",
    "        score = score.transpose(0,1)      \n",
    "        \n",
    "        loss = criterion(score, label)\n",
    "        losses.update(loss.item())\n",
    "        \n",
    "        pred = score.argmax(1)\n",
    "        acc = pred.eq(target).sum().item()   \n",
    "        acces.update(acc)\n",
    "        break\n",
    "        print(f'epoch:{epoch}, idx:{idx}/{len(data_iter)}, loss:{losses.avg}, acc:{acces.avg}')\n",
    "    return losses.avg, acces.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T07:36:11.237414Z",
     "start_time": "2019-01-03T07:36:11.223826Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                             lr=config.lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T07:36:14.488360Z",
     "start_time": "2019-01-03T07:36:12.041865Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzs/Multi_Evidence_QA/model.py:150: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alphas = self.softmax(alphas)  # (bsz, sent_len)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.9267146587371826, 0.0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(config.epochs):\n",
    "    train_loss, train_acc = train(epoch, train_iter, model, criterion, optimizer, False)\n",
    "    val_loss, val_acc = val(epoch, val_iter, model, criterion, False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T07:44:01.543880Z",
     "start_time": "2019-01-03T07:44:01.536501Z"
    }
   },
   "outputs": [],
   "source": [
    "save_path = config.model_name + '_epoch'+str(config.epochs) + '_lr'+ str(config.lr)\n",
    "save_path = os.path.join(config.log_dir, save_path)\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
