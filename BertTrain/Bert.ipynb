{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T13:43:09.636923Z",
     "start_time": "2019-03-04T13:43:08.717850Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T14:17:34.674825Z",
     "start_time": "2019-03-04T14:17:33.888677Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikiExample(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 wiki_id,\n",
    "                 passages,\n",
    "                 choices,\n",
    "                 question,\n",
    "                 label=None):\n",
    "        self.wiki_id = wiki_id\n",
    "        self.passages = passages\n",
    "        self.choices = choices\n",
    "        self.label = label\n",
    "        self.question = question\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        l = [\n",
    "            f'id: {self.wiki_id}',\n",
    "            f'question: {self.question}',\n",
    "            f'passages:[{\",\".join(self.passages)}]',\n",
    "            f'choices:[{\",\".join(self.choices)}]',\n",
    "        ]\n",
    "        \n",
    "        if self.label is not None:\n",
    "            l.append(f'label: {self.label}')\n",
    "        \n",
    "        return '\\n'.join(l)\n",
    "        \n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        \n",
    "class WikiFeatures(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 wiki_id,\n",
    "                 passages_features,\n",
    "                 choices_features,\n",
    "                 question_features,\n",
    "                 label=None):\n",
    "        \n",
    "        self.passages_features = passages_features\n",
    "        self.choices_features = choices_features\n",
    "        self.question_features = question_features\n",
    "        self.label = label\n",
    "        self.example_id = wiki_id \n",
    "        \n",
    "def read_wiki_example(path):\n",
    "    data = json.load(open(path))\n",
    "    examples = []\n",
    "    for item in data:\n",
    "        passages = item['supports']\n",
    "        choices = item['candidates']\n",
    "        question = item['query']\n",
    "        wiki_id = item['id']\n",
    "        answer = item['answer']\n",
    "        label = -1\n",
    "        for idx, choice in enumerate(choices):\n",
    "            if choice == answer:\n",
    "                label = idx\n",
    "        if label == -1:\n",
    "            print(wiki_id)\n",
    "            continue\n",
    "        example = WikiExample(wiki_id, passages, choices, question, label)\n",
    "        examples.append(example)\n",
    "    return examples        \n",
    "\n",
    "def _truncate_seq(seq, max_seq_length):\n",
    "    if len(seq) <= max_seq_length:\n",
    "        return seq\n",
    "    else:\n",
    "        return seq[:max_seq_length]\n",
    "    \n",
    "def convert_context_to_features(context, max_seq_length=None):\n",
    "    context_token = tokenizer.tokenize(context)\n",
    "    if max_seq_length is None:\n",
    "        max_seq_length = len(context_token) + 2\n",
    "    context_token = _truncate_seq(context_token, max_seq_length - 2)\n",
    "    context_token = [\"[CLS]\"] + context_token + [\"[SEP]\"] \n",
    "\n",
    "    segment_ids = [0]*len(context_token)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(context_token)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    padding = [0] * (max_seq_length - len(context_token))\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    segment_ids += padding\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    return InputFeatures(input_ids, input_mask, segment_ids)    \n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, max_seq_length):\n",
    "    all_features = []\n",
    "    for example in tqdm(examples):\n",
    "        max_choice_len = max([ len(tokenizer.tokenize(choice)) for choice in example.choices])\n",
    "\n",
    "        choices_features = [convert_context_to_features(choice, max_choice_len+2) for choice in example.choices]\n",
    "        passages_features = [convert_context_to_features(passage, max_seq_length) for passage in example.passages]\n",
    "        question_features = [convert_context_to_features(example.question)]\n",
    "        \n",
    "        all_features.append(WikiFeatures(example.wiki_id,\n",
    "                                         passages_features,\n",
    "                                         choices_features,\n",
    "                                         question_features,\n",
    "                                         example.label))\n",
    "    return all_features\n",
    "    \n",
    "\n",
    "def convert_to_tensor(input_features):\n",
    "    input_ids = torch.tensor([features.input_ids for features in input_features], dtype=torch.long)\n",
    "    input_mask = torch.tensor([features.input_mask for features in input_features], dtype=torch.long)\n",
    "    segment_ids = torch.tensor([features.segment_ids for features in input_features], dtype=torch.long)\n",
    "    return input_ids, input_mask, segment_ids   \n",
    "\n",
    "def make_input(item):\n",
    "    d = {}\n",
    "    d['choices'] = convert_to_tensor(item.choices_features)\n",
    "    d['question'] = convert_to_tensor(item.question_features)\n",
    "    d['passages'] = convert_to_tensor(item.passages_features)\n",
    "    d['label'] = torch.tensor([item.label], dtype=torch.long)\n",
    "    d['wiki_id'] = item.example_id\n",
    "    return d    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T14:17:34.773829Z",
     "start_time": "2019-03-04T14:17:34.677663Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/04/2019 22:17:34 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file ./bert-base-uncased-vocab.txt\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./bert-base-uncased-vocab.txt', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T14:17:43.771036Z",
     "start_time": "2019-03-04T14:17:43.763495Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_json_path = './data/qangaroo_v1.1/wikihop/train.json'\n",
    "dev_json_path = './data/qangaroo_v1.1/wikihop/dev.json'\n",
    "\n",
    "max_seq_length = 128\n",
    "\n",
    "train_bert_path = f'./data/train_data_bert_{max_seq_length}.pt'\n",
    "dev_bert_path = f'./data/dev_data_bert_{max_seq_length}.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T15:04:32.539505Z",
     "start_time": "2019-03-04T14:17:44.392790Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43738/43738 [40:39<00:00, 16.77it/s]  \n",
      "100%|██████████| 5129/5129 [05:06<00:00, 16.73it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_examples  = read_wiki_example(train_json_path)\n",
    "dev_examples = read_wiki_example(dev_json_path)\n",
    "\n",
    "train_features = convert_examples_to_features(train_examples, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
    "dev_features = convert_examples_to_features(dev_examples, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
    "\n",
    "train_data = [make_input(f) for f in train_features]\n",
    "dev_data = [make_input(f) for f in dev_features]\n",
    "\n",
    "torch.save(train_data, train_bert_path)\n",
    "torch.save(dev_data, dev_bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T14:17:08.866024Z",
     "start_time": "2019-03-04T14:17:08.860333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accounting',\n",
       " 'artist',\n",
       " 'band',\n",
       " 'barber',\n",
       " 'canada',\n",
       " 'commercial',\n",
       " 'indie pop',\n",
       " 'manufacturer',\n",
       " 'marketing',\n",
       " 'scouting',\n",
       " 'singer',\n",
       " 'united kingdom']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[9].choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T09:11:02.870111Z",
     "start_time": "2019-03-01T09:10:21.333643Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bert_path = f'./data/train_data_bert.pt'\n",
    "dev_bert_path = f'./data/dev_data_bert.pt'\n",
    "train_data = torch.load(train_bert_path)\n",
    "dev_data = torch.load(dev_bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T09:11:23.027854Z",
     "start_time": "2019-03-01T09:11:22.926388Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_nums = []\n",
    "for item in train_data:\n",
    "    passages = item['passages']\n",
    "    p_nums.append(passages[0].size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T09:11:25.374098Z",
     "start_time": "2019-03-01T09:11:23.603090Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T11:55:36.487468Z",
     "start_time": "2019-03-01T11:55:36.467499Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(p_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T11:55:37.848638Z",
     "start_time": "2019-03-01T11:55:37.828895Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T09:12:54.798440Z",
     "start_time": "2019-03-01T09:12:54.791517Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df > 32] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T11:55:45.391955Z",
     "start_time": "2019-03-01T11:55:45.381573Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[0].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
